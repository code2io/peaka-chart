nameOverride: ""
fullnameOverride: ""

imagePullSecrets: []

domain: "localhost"
port: 80

# Create a public & private key pair and feed them to here
# generate a private key with `openssl genrsa -out key.pem 2048`
# generate a public key with `openssl rsa -in key.pem -outform PEM -pubout -out key.pem.pub`
jwtRsaPublicKey: "-----BEGIN PUBLIC KEY-----
MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAnQco6kt2qyolT7DwsGFA
EBdMi6Vt/QZ6WdoqqrKFKDF4FUQFbyjSF9pJ0B6LCUbuxdAOn3y9Ju5Rp3wAlIMh
3vSU8YJm61Tk7K3hCdlgnD0DJZ7PSeTiSYGVljf4Vmr2lX04wvLlTvj1WkAEddOX
bLK1n21REGZyuIm7bw+sv2KpnP+LA5BhDsdqDAiqSNwlvFmnACpGr3dkWdIEwrmh
16KYbQX4wKYOTjZQCHdwGHNiEc37kcN4x/n8XdlGC65ssTRARy2Yx5kk8pCwsDcI
jH0r4ct4ss4kesCFnB/A2pbe922VlMVs8YZYKTuGDcGRYvcn3r/YVCEfx7EuhsQC
IwIDAQAB
-----END PUBLIC KEY-----"
jwtRsaPrivateKey: "-----BEGIN RSA PRIVATE KEY-----
MIIEowIBAAKCAQEAnQco6kt2qyolT7DwsGFAEBdMi6Vt/QZ6WdoqqrKFKDF4FUQF
byjSF9pJ0B6LCUbuxdAOn3y9Ju5Rp3wAlIMh3vSU8YJm61Tk7K3hCdlgnD0DJZ7P
SeTiSYGVljf4Vmr2lX04wvLlTvj1WkAEddOXbLK1n21REGZyuIm7bw+sv2KpnP+L
A5BhDsdqDAiqSNwlvFmnACpGr3dkWdIEwrmh16KYbQX4wKYOTjZQCHdwGHNiEc37
kcN4x/n8XdlGC65ssTRARy2Yx5kk8pCwsDcIjH0r4ct4ss4kesCFnB/A2pbe922V
lMVs8YZYKTuGDcGRYvcn3r/YVCEfx7EuhsQCIwIDAQABAoIBAHD7X9MELQQGtQXb
t7IQpvls6iRoCPVPHeVcdeqPvAMpZM9YO9i5NED5TLaz4zHvmYk2o+7Y33gNbS7m
p7wzhcFXlXsXalnUoUox6YMepsaDl6oP0/HE/5QH6stExifXmkgA60BJZ/gkZNRk
z1C/x2nsQ4XML9FvBQgGAZhtXMvuXnN4/Nicv9w+XgPNBFPdiPclcwonWA7VMDga
o0BRJVmAVaEq8BDyQY4Jm32q0EXmPKd5DYCWsEQWPBZYjUsgQUKL67/bextw4XcR
Ddw5Q4GnkYJCA730Odo8huJaLzWmmypkiAZTimG7Gl8KTZE0v+g6XdC3TwQ/xifn
hYMdcpECgYEA0XiO0bMZRXxVjb2VB8uLo4ODYRNL92b4hLNgc58weGekzH0oF8QT
6kjuh/szqmMgVE8IYO2ENWxQnxbMpLLUBX3RgpzEuEX1zhjMKx+zRfu0GDNA9OIK
oqhxCWI6V3h85tG/C5ZlzoZ9LM9xV3k6yJDfGJKAsylj6N/d6OMRKX0CgYEAv+h2
rB+8kGNoELIRkHBL47ZwyDPCKh166mSCSrVYo5vOKAFHqXsem8LxPA1cXbeMLMXg
/l0gPK/fsnHhfkQ5rioeDSMJK0JNnDC83J4Jpue4TkXRX91b9H2jLw2k0c5NhNRS
+1YIZzhENDXKrZmmV71gF31iUNnrHrzA0QgQrB8CgYAg+BOqh8DjgeVJUNyDBJNu
4gUso0fioa9d24nDyPttCAsVMGG+E9t6lY4NkwkXuVBeVK4h62BStECgOYtuJMNg
NeRi5V/FDSXaEWcZJn0l967p3C9OG8HGxjI8YtyB5fHqtnqWTU6qydor0l6pgOgy
pzKmAHEHQe16urmHrS2HVQKBgGPWnX6YmDIJ9UbSTP723RRfgrc07iuY4Kx2k64o
QDAFy6/WUqDic58XQQc78DS1W9CjNZU/f5jy6o7cXjKhbdyk/rNY6dk+ij/avzaL
ZXwWizT1b6LO95rvjejZ0UhxarTS/UTSklaY2CrNXV8JU5fL5uTxjyd+56o7Cpj0
+C+DAoGBAMDObugNucMjf0kAZ9bgq4kebaVu1QeboRGiKxz/x/r+TvrzQMS5ghsD
Rgnj/MpOfHaX4fAv9DI5lx7qnQ64713VXeqNgBbGxKshYV/Q63/o/x6O8h6aN9X9
gYuM1GcRepbRjEX0UMJu8Bo4ndw6BA2UdPc7q0NN7hu27lyQ3DqM
-----END RSA PRIVATE KEY-----"

openAIApiKey:

connector:
  credentials:
    enabled: true

# TLS settings
tls:
  enabled: false
  cert: ""
  key: ""

liquibase:
  image:
    name: "be-data-migrator"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"

dbc:
  port: 4567

# disable this if you want to use an external postgresql server
postgresql:
  enabled: true
  volumePermissions:
    enabled: true
  auth:
    username: "code2db"
    password: "code2db"
    database: "code2db"
  primary:
    extendedConfiguration: |-
      idle_session_timeout = 600000
      max_connections = 1000
    persistence:
      size: 2Gi

# disable this if you want to use an external nfs server
nfsServer:
  enabled: true
  # set this if you are using an external nfs server
  nfsServerAddress: ""
  persistentVolume:
    # pv size of the nfs server
    size: 2Gi
    # keep the nfs pvc on helm uninstalls
    volumeDeletionProtection: true
    # path to be used as the nfs share
    mountPath: /nfsshare
  deployment:
    # it is our recommendation that you run nfs server as root, otherwise is not tested
    containerSecurityContext:
      enabled: true
      privileged: true
    containerPort: 2049
  servicePort: 2049

# nfs share to be used across some Peaka pods
nfsShare:
  # keep the nfs pvc on helm uninstalls
  volumeDeletionProtection: true
  # size of the nfs share
  size: 2Gi
  # what happens to a persistent volume when released from its claim (Retain/Delete)
  persistentVolumeReclaimPolicy: Retain
  # path that is exported by the NFS server
  path: "/"

# thrift store
hiveMetastore:
  enabled: true
  image:
    repository: bitsondatadev/hive-metastore
    tag: latest
    pullPolicy: IfNotPresent
  hadoopHeapSize: "10240"
  # db type to be used as metastore. One of mysql or postgres (only mysql tested)
  metastoreType: mysql
  servicePort: 9083
  # by default, hive metastore connects to minio using default minio user. If you want to change this,
  # create a user by entering accessKey, secretKey and policy in minio.users, then change below two values accordingly.
  minioAccessKey: ""
  minioSecretKey: ""

# s3 compatible object storage configuration. By default, Peaka uses MinIO.
# for the full list of values, see https://github.com/minio/minio/tree/master/helm/minio
minio:
  # do not disable as Peaka is not yet tested with other S3 storage options.
  enabled: true
  mode: standalone
  replicas: 1
  persistence:
    size: 2Gi

mariadb:
  enabled: true
  replicaCount: 1
  db:
    user: peaka
    password: peaka
    name: metastore_db
  rootUser:
    password: peaka
  galera:
    mariabackup:
      password: peaka
  persistence:
    size: 2Gi

kafka:
  # do not disable as Peaka is not yet tested with other MQ options.
  enabled: true
  provisioning:
    numPartitions: 20
  extraConfig: |-
    log.retention.hours=12
    max.message.bytes=50000000
    delete.topic.enable=true
  volumePermissions:
    enabled: true
  controller:
    replicaCount: 1
    persistence:
      size: 2Gi

mongodb:
  # do not disable as Peaka requires a MongoDB server to be present in order to operate correctly.
  enabled: true
  architecture: standalone
  useStatefulSet: true
  persistence:
    size: 2Gi

redis:
  enabled: true
  auth:
    enabled: false
  architecture: standalone
  master:
    persistence:
      size: 2Gi

postgresqlbigtable:
  enabled: true
  volumePermissions:
    enabled: true
  auth:
    username: "code2db"
    password: "code2db"
    database: "code2db"
  primary:
    extendedConfiguration: |-
      idle_session_timeout = 600000
      max_connections = 1000
    persistence:
      size: 2Gi

# Default values for cp-kafka-connect.
kafkaConnect:
  enabled: false

  replicaCount: 1

  ## Image Info
  ## ref: https://hub.docker.com/r/confluentinc/cp-kafka/
  image: debezium/connect
  imageTag: 2.5

  ## Specify a imagePullPolicy
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  imagePullPolicy: IfNotPresent

  ## Specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
  imagePullSecrets:

  servicePort: 8083

  ## Kafka Connect properties
  ## ref: https://docs.confluent.io/current/connect/userguide.html#configuring-workers
  configurationOverrides:
    "plugin.path": "/usr/share/java,/usr/share/confluent-hub-components"
    "key.converter": "io.confluent.connect.avro.AvroConverter"
    "value.converter": "io.confluent.connect.avro.AvroConverter"
    "key.converter.schemas.enable": "false"
    "value.converter.schemas.enable": "false"
    "internal.key.converter": "org.apache.kafka.connect.json.JsonConverter"
    "internal.value.converter": "org.apache.kafka.connect.json.JsonConverter"
    "config.storage.replication.factor": "3"
    "offset.storage.replication.factor": "3"
    "status.storage.replication.factor": "3"

  ## Kafka Connect JVM Heap Option
  heapOptions: "-Xms512M -Xmx512M"

  ## Additional env variables
  ## CUSTOM_SCRIPT_PATH is the path of the custom shell script to be ran mounted in a volume
  customEnv: { }
  # CUSTOM_SCRIPT_PATH: /etc/scripts/create-connectors.sh

  resources: { }
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #  cpu: 100m
    #  memory: 128Mi
    # requests:
  #  cpu: 100m
  #  memory: 128Mi

  ## Custom pod annotations
  podAnnotations: { }

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: { }

  ## Taints to tolerate on node assignment:
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: [ ]

  ## Pod scheduling constraints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: { }

  ## If the Kafka Chart is disabled a URL and port are required to connect
  ## e.g. gnoble-panther-cp-schema-registry:8081
  cp-schema-registry:
    url: ""

  ## List of volumeMounts for connect server container
  ## ref: https://kubernetes.io/docs/concepts/storage/volumes/
  volumeMounts:
  # - name: credentials
  #   mountPath: /etc/creds-volume

  ## List of volumeMounts for connect server container
  ## ref: https://kubernetes.io/docs/concepts/storage/volumes/
  volumes:
  # - name: credentials
  #   secret:
  #     secretName: creds

  ## Secret with multiple keys to serve the purpose of multiple secrets
  ## Values for all the keys will be base64 encoded when the Secret is created or updated
  ## ref: https://kubernetes.io/docs/concepts/configuration/secret/
  secrets:
  # username: kafka123
  # password: connect321

  ## These values are used only when "customEnv.CUSTOM_SCRIPT_PATH" is defined.
  ## "livenessProbe" is required only for the edge cases where the custom script to be ran takes too much time
  ## and errors by the ENTRYPOINT are ignored by the container
  ## As an example such a similar script is added to "cp-helm-charts/examples/create-connectors.sh"
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  livenessProbe:
  # httpGet:
  #   path: /connectors
  #   port: 8083
  # initialDelaySeconds: 30
  # periodSeconds: 5
  # failureThreshold: 10

# vector db for postgresql
pgvector:
  enabled: true
  db:
    name: vectordb
    schema: studio
    user: vectordb
    password: vectordb
  persistence:
    size: 8Gi
  image:
    repository: "ankane/pgvector"
    version: v0.5.1
  options:
    maxConnections: 1000
    sharedBuffers: 1024MB
  port: 5432
  replicaCount: 1
  extraEnvVars: []

# peaka relies on temporal for its workflow executions
temporal:
  enabled: true
  manager:
    resources:
      limits:
        memory: 256Mi
  temporalCluster:
    version: 1.23.0
    numHistoryShards: 1
    ## by default, peaka uses the default installed postgresql (dependency chart) for default store and visibility
    ## store of temporal. Fill below values if you want to use another database.
    ## See https://github.com/alexandrevilain/temporal-operator/tree/main for details.
    persistence:
      defaultStore:
        dbPlugin: ""
        dbUser: ""
        dbHostName: ""
        dbPort: ""
        passwordSecretName: ""
        passwordSecretKey: ""
      visibilityStore:
        dbPlugin: ""
        dbUser: ""
        dbHostName: ""
        dbPort: ""
        passwordSecretName: ""
        passwordSecretKey: ""

# peaka uses trino for sql engine
trino:
  enabled: true
  fullnameOverride: ""
  nameOverride: ""
  workerNameOverride: ""
  coordinatorNameOverride: ""
  additionalConfigProperties:
    #    - retry-policy=QUERY
    - catalog.management=DYNAMIC
  additionalNodeProperties: {}
  additionalLogProperties: {}
  additionalExchangeManagerProperties: {}
  eventListenerProperties: {}
  additionalCatalogs: {}
  tcpRouteEnabled: true
  accessControl: {}
  image:
    name: trino
    pullPolicy: IfNotPresent
    tag: prod
  initContainers: {}
  auth: {}
    # Set username and password
    # https://trino.io/docs/current/security/password-file.html#file-format
    # passwordAuth: "username:encrypted-password-with-htpasswd"
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
  coordinator:
    config:
      query:
        maxMemoryPerNode: "9GB"
        maxLength: "1000000000"     # a billion
      memory:
        heapHeadroomPerNode: ""
    jvm:
      maxHeapSize: "25G"
      gcMethod:
        type: "UseG1GC"
        g1:
          heapRegionSize: "32M"
    additionalJVMConfig:
      - -Dfile.encoding=UTF-8
      - --add-opens=java.base/java.nio=ALL-UNNAMED
    additionalExposedPorts: {}
    resources: {}
      # We usually recommend not to specify default resources and to leave this as a conscious
      # choice for the user. This also increases chances charts run on environments with little
      # resources, such as Minikube. If you do want to specify resources, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      # limits:
      #   cpu: 100m
      #   memory: 128Mi
      # requests:
      #   cpu: 100m
      #   memory: 128Mi
    livenessProbe: {}
      # initialDelaySeconds: 20
      # periodSeconds: 10
      # timeoutSeconds: 5
      # failureThreshold: 6
      # successThreshold: 1
    readinessProbe: {}
      # initialDelaySeconds: 20
      # periodSeconds: 10
      # timeoutSeconds: 5
      # failureThreshold: 6
      # successThreshold: 1
    nodeSelector: {}
    tolerations: []
    affinity: {}
    additionalConfigFiles: { }
  worker:
    config:
      memory:
        heapHeadroomPerNode: ""
      query:
        maxMemoryPerNode: "15GB"
        maxLength: "1000000000"     # a billion
    jvm:
      maxHeapSize: "50G"
      gcMethod:
        type: "UseG1GC"
        g1:
          heapRegionSize: "32M"
    additionalJVMConfig:
      - -Dfile.encoding=UTF-8
      - --add-opens=java.base/java.nio=ALL-UNNAMED
    additionalConfigFiles: {}
    additionalExposedPorts: {}
    resources: {}
    livenessProbe: {}
      # initialDelaySeconds: 20
      # periodSeconds: 10
      # timeoutSeconds: 5
      # failureThreshold: 6
      # successThreshold: 1
    readinessProbe: {}
      # initialDelaySeconds: 20
      # periodSeconds: 10
      # timeoutSeconds: 5
      # failureThreshold: 6
      # successThreshold: 1
    nodeSelector: {}
    tolerations: []
    affinity: {}
  server:
    exchangeManager:
      name: "filesystem"
      baseDir: "/tmp/trino-local-file-system-exchange-manager"
    autoscaling:
      enabled: false
      targetCPUUtilizationPercentage: 80
      maxReplicas: 3
    log:
      trino:
        level: INFO
    config:
      path: /etc/trino
      query:
        maxMemory: "20GB"
      authenticationType: ""
      https:
        enabled: false
        port: 8443
        keystore:
          path: ""
    workers: 1
    node:
      environment: production
      dataDir: /data/trino
      pluginDir: /usr/lib/trino/plugin
    coordinatorExtraConfig: ""
    workerExtraConfig: ""
  nodePort: 32181
  service:
    type: ClusterIP
    port: 8080
  serviceAccount:
    # Specifies whether a service account should be created
    create: false
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""
    # Annotations to add to the service account
    annotations: {}

aiRest:
  enabled: true
  replicaCount: 1
  image:
    name: "be-ai-rest"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  tolerations: {}

authService:
  enabled: true
  replicaCount: 1
  image:
    name: "be-auth-service"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

cloudGateway:
  enabled: true
  replicaCount: 1
  image:
    name: "be-cloud-gateway"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}

collabSharedb:
  enabled: true
  replicaCount: 1
  image:
    name: "be-collab-sharedb"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 15
    successThreshold: 1
    timeoutSeconds: 15

dataCache:
  enabled: true
  replicaCount: 1
  image:
    name: "be-data-cache"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  sidecar:
    imageName: "be-common-flow-utilities-service"
    imageTag: "prod"
    imagePullPolicy: "IfNotPresent"
    resources: {}
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 45
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5

dataRest:
  enabled: true
  replicaCount: 1
  image:
    name: "be-data-rest"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}

dispatcher:
  enabled: true
  replicaCount: 1
  image:
    name: "be-dispatcher"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}

dispatcherAssigner:
  enabled: true
  replicaCount: 1
  image:
    name: "be-dispatcher-assigner"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}

dispatcherDlq:
  enabled: true
  replicaCount: 1
  image:
    name: "be-dispatcher-dlq"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}

emailService:
  enabled: true
  replicaCount: 1
  image:
    name: "be-email-service"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}

metadataService:
  enabled: true
  replicaCount: 1
  image:
    name: "be-metadata-service"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

monitoringService:
  enabled: true
  replicaCount: 1
  image:
    name: "be-monitoring-service"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

permissionService:
  enabled: true
  replicaCount: 1
  image:
    name: "be-permission-service"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

runtimeApi:
  enabled: true
  replicaCount: 1
  image:
    name: "be-runtime-api"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

scheduledFlowRunner:
  enabled: true
  replicaCount: 1
  image:
    name: "be-scheduled-flow-runner"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5


secretStoreService:
  enabled: true
  replicaCount: 1
  secretEncryptionKey: "key"
  image:
    name: "be-secret-store-service"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

studioApi:
  enabled: true
  replicaCount: 1
  image:
    name: "be-studio-api"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

tokenService:
  enabled: true
  replicaCount: 1
  image:
    name: "be-token-service"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

webhookResolver:
  enabled: true
  replicaCount: 1
  image:
    name: "be-webhook-resolver"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}

workflowHistory:
  enabled: true
  replicaCount: 1
  image:
    name: "be-workflow-history"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

workflowStarter:
  enabled: true
  replicaCount: 1
  image:
    name: "be-workflow-starter"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}

workflowWorkerExpress:
  enabled: true
  replicaCount: 1
  image:
    name: "be-workflow-worker-express"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  sidecar:
    imageName: "be-common-flow-utilities-service"
    imageTag: "prod"
    imagePullPolicy: "IfNotPresent"
    resources: {}
    readinessProbe: {}
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

studioWeb:
  enabled: true
  replicaCount: 1
  image:
    name: "fe-studio-app"
    tag: "prod"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe: {}
