nameOverride: ""
fullnameOverride: ""

# disable this if you want to use an external postgresql server
postgresql:
  enabled: true
  volumePermissions:
    enabled: true
  auth:
    username: "code2db"
    password: "code2db"
    database: "code2db"
  primary:
    extendedConfiguration: |-
      idle_session_timeout = 600000
      max_connections = 1000

# disable this if you want to use an external nfs server
nfsServer:
  enabled: true
  # set this if you are using an external nfs server
  nfsServerAddress: ""
  persistentVolume:
    # pv size of the nfs server
    size: 8Gi
    # keep the nfs pvc on helm uninstalls
    volumeDeletionProtection: true
    # path to be used as the nfs share
    mountPath: /nfsshare
  deployment:
    # it is our recommendation that you run nfs server as root, otherwise is not tested
    containerSecurityContext:
      enabled: true
      privileged: true
    containerPort: 2049
  servicePort: 2049

# nfs share to be used across some Peaka pods
nfsShare:
  # keep the nfs pvc on helm uninstalls
  volumeDeletionProtection: true
  # size of the nfs share
  size: 8Gi
  # what happens to a persistent volume when released from its claim (Retain/Delete)
  persistentVolumeReclaimPolicy: Retain
  # path that is exported by the NFS server
  path: "/"

# thrift store
hiveMetastore:
  enabled: true
  image:
    repository: bitsondatadev/hive-metastore
    tag: latest
    pullPolicy: IfNotPresent
  hadoopHeapSize: "10240"
  # db type to be used as metastore. One of mysql or postgres (only mysql tested)
  metastoreType: mysql
  servicePort: 9083
  # by default, hive metastore connects to minio using default minio user. If you want to change this,
  # create a user by entering accessKey, secretKey and policy in minio.users, then change below two values accordingly.
  minioAccessKey: ""
  minioSecretKey: ""

# s3 compatible object storage configuration. By default, Peaka uses MinIO.
# for the full list of values, see https://github.com/minio/minio/tree/master/helm/minio
minio:
  # do not disable as Peaka is not yet tested with other S3 storage options.
  enabled: true
  mode: standalone
  replicas: 1

mariadb:
  enabled: true
  replicaCount: 1
  db:
    user: peaka
    password: peaka
    name: metastore_db

kafka:
  # do not disable as Peaka is not yet tested with other MQ options.
  enabled: true
  numPartitions: 20
  deleteTopicEnable: true
  maxMessageBytes: "50000000"
  logRetentionHours: 12
  volumePermissions:
    enabled: true

mongodb:
  # do not disable as Peaka requires a MongoDB server to be present in order to operate correctly.
  enabled: true
  architecture: standalone
  useStatefulSet: true

redis:
  enabled: true
  auth:
    enabled: false
  architecture: standalone

postgresqlBigtable:
  enabled: true
  volumePermissions:
    enabled: true
  auth:
    username: "code2db"
    password: "code2db"
    database: "code2db"
  primary:
    extendedConfiguration: |-
      idle_session_timeout = 600000
      max_connections = 1000

# Default values for cp-kafka-connect.
kafkaConnect:
  enabled: true

  replicaCount: 1

  ## Image Info
  ## ref: https://hub.docker.com/r/confluentinc/cp-kafka/
  image: debezium/connect
  imageTag: 2.5

  ## Specify a imagePullPolicy
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  imagePullPolicy: IfNotPresent

  ## Specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
  imagePullSecrets:

  servicePort: 8083

  ## Kafka Connect properties
  ## ref: https://docs.confluent.io/current/connect/userguide.html#configuring-workers
  configurationOverrides:
    "plugin.path": "/usr/share/java,/usr/share/confluent-hub-components"
    "key.converter": "io.confluent.connect.avro.AvroConverter"
    "value.converter": "io.confluent.connect.avro.AvroConverter"
    "key.converter.schemas.enable": "false"
    "value.converter.schemas.enable": "false"
    "internal.key.converter": "org.apache.kafka.connect.json.JsonConverter"
    "internal.value.converter": "org.apache.kafka.connect.json.JsonConverter"
    "config.storage.replication.factor": "3"
    "offset.storage.replication.factor": "3"
    "status.storage.replication.factor": "3"

  ## Kafka Connect JVM Heap Option
  heapOptions: "-Xms512M -Xmx512M"

  ## Additional env variables
  ## CUSTOM_SCRIPT_PATH is the path of the custom shell script to be ran mounted in a volume
  customEnv: { }
  # CUSTOM_SCRIPT_PATH: /etc/scripts/create-connectors.sh

  resources: { }
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #  cpu: 100m
    #  memory: 128Mi
    # requests:
  #  cpu: 100m
  #  memory: 128Mi

  ## Custom pod annotations
  podAnnotations: { }

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: { }

  ## Taints to tolerate on node assignment:
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: [ ]

  ## Pod scheduling constraints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: { }

  ## If the Kafka Chart is disabled a URL and port are required to connect
  ## e.g. gnoble-panther-cp-schema-registry:8081
  cp-schema-registry:
    url: ""

  ## List of volumeMounts for connect server container
  ## ref: https://kubernetes.io/docs/concepts/storage/volumes/
  volumeMounts:
  # - name: credentials
  #   mountPath: /etc/creds-volume

  ## List of volumeMounts for connect server container
  ## ref: https://kubernetes.io/docs/concepts/storage/volumes/
  volumes:
  # - name: credentials
  #   secret:
  #     secretName: creds

  ## Secret with multiple keys to serve the purpose of multiple secrets
  ## Values for all the keys will be base64 encoded when the Secret is created or updated
  ## ref: https://kubernetes.io/docs/concepts/configuration/secret/
  secrets:
  # username: kafka123
  # password: connect321

  ## These values are used only when "customEnv.CUSTOM_SCRIPT_PATH" is defined.
  ## "livenessProbe" is required only for the edge cases where the custom script to be ran takes too much time
  ## and errors by the ENTRYPOINT are ignored by the container
  ## As an example such a similar script is added to "cp-helm-charts/examples/create-connectors.sh"
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  livenessProbe:
  # httpGet:
  #   path: /connectors
  #   port: 8083
  # initialDelaySeconds: 30
  # periodSeconds: 5
  # failureThreshold: 10
